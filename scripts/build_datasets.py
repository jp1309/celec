#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CELEC (Ecuador) – Hidrología & Producción (CSR) – Build dashboard datasets from monthly CSV drops.

Repo layout expected (repo root: /celec):
  - Hidro_mensual/*.csv
  - Produ_mensual/*.csv

Outputs (generated by this script):
  - data/produccion_diaria_larga.csv
  - data/hidrologia_diaria_larga.csv
  - data/meta.json

Rules (per Juan Pablo):
  - Energy is MWh. Flows are m3/s. Cotas are msnm.
  - Data starts 2020-01.
  - December 2025 may be partial. When a daily row has ALL zeros for the component plants
    (Molino, Mazar, Sopladora, MSF), treat that day as "no data": keep the date but set values to NA,
    and mark is_placeholder=1 so it won't appear as a real zero in charts.
  - File structure is stable.
  - EnergiaCsr is the SUM of Molino + Mazar + Sopladora + MSF. We will compute CSR ourselves.
    (We still accept an EnergiaCsr column in the input, but we do not rely on it.)
"""

from __future__ import annotations

import json
import datetime as dt
from pathlib import Path
from typing import List

import pandas as pd

REPO_ROOT = Path(__file__).resolve().parents[1]
PROD_DIR = REPO_ROOT / "Produ_mensual"
HIDRO_DIR = REPO_ROOT / "Hidro_mensual"
OUT_DIR = REPO_ROOT / "data"
OUT_DIR.mkdir(parents=True, exist_ok=True)

START_DATE = pd.Timestamp("2020-01-01")

# Production columns (components)
PROD_COMPONENTS = {
    "EnergiaMol": "molino",
    "EnergiaMaz": "mazar",
    "EnergiaSop": "sopladora",
    "EnergiaMsf": "msf",
}
# Optional input column (we don't rely on it)
PROD_CSR_INPUT = "EnergiaCsr"

# Hydro columns
HIDRO_COLS = {
    "CaudalCuencaPaute": ("cuenca_paute", "caudal_m3s"),
    "CaudalMol": ("molino", "caudal_m3s"),
    "CotaMol": ("molino", "cota_msnm"),
    "CaudalMaz": ("mazar", "caudal_m3s"),
    "CotaMaz": ("mazar", "cota_msnm"),
    "CaudalSop": ("sopladora", "caudal_m3s"),
    "CotaSop": ("sopladora", "cota_msnm"),
    "CaudalMsf": ("msf", "caudal_m3s"),
    "CotaMsf": ("msf", "cota_msnm"),
}

def _read_csv(path: Path) -> pd.DataFrame:
    # Robust to UTF-8 with BOM and latin-1.
    try:
        df = pd.read_csv(path, encoding="utf-8-sig")
    except Exception:
        df = pd.read_csv(path, encoding="latin1")
    df.columns = [c.replace("ï»¿", "").strip() for c in df.columns]
    return df

def _parse_fecha(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    # Input format: DD/MM/YYYY HH:MM:SS
    df["Fecha"] = pd.to_datetime(df["Fecha"], dayfirst=True, errors="coerce")
    return df

def _is_all_zero_row(row: pd.Series, cols: List[str]) -> bool:
    # Placeholder day = all component columns exist and are exactly 0 (not NaN).
    try:
        vals = row[cols].astype(float)
        return (vals == 0).all() and vals.notna().all()
    except Exception:
        return False

def build_produccion_diaria_larga() -> pd.DataFrame:
    files = sorted(PROD_DIR.glob("*.csv"))
    if not files:
        raise FileNotFoundError(f"No CSV files found in {PROD_DIR}")

    parts = []
    comp_cols = list(PROD_COMPONENTS.keys())

    for f in files:
        df = _read_csv(f)
        df = _parse_fecha(df)

        required = ["Fecha", *comp_cols]
        missing = [c for c in required if c not in df.columns]
        if missing:
            raise ValueError(f"{f.name} missing columns: {missing}")

        # Placeholder detection uses ONLY component plants (not CSR)
        df["is_placeholder"] = df.apply(lambda r: 1 if _is_all_zero_row(r, comp_cols) else 0, axis=1)

        # Compute CSR from components (sum). If placeholder => NA.
        df["EnergiaCsr_calc"] = df[comp_cols].sum(axis=1, numeric_only=True)
        df.loc[df["is_placeholder"] == 1, "EnergiaCsr_calc"] = pd.NA
        for c in comp_cols:
            df.loc[df["is_placeholder"] == 1, c] = pd.NA

        # Build long: components + csr (computed)
        long_parts = []

        # Components
        long_comp = df[["Fecha", "is_placeholder", *comp_cols]].melt(
            id_vars=["Fecha", "is_placeholder"],
            var_name="variable",
            value_name="energia_mwh",
        )
        long_comp["central"] = long_comp["variable"].map(PROD_COMPONENTS)
        long_comp.drop(columns=["variable"], inplace=True)
        long_parts.append(long_comp)

        # CSR
        csr = df[["Fecha", "is_placeholder", "EnergiaCsr_calc"]].copy()
        csr.rename(columns={"EnergiaCsr_calc": "energia_mwh"}, inplace=True)
        csr["central"] = "csr"
        long_parts.append(csr)

        long = pd.concat(long_parts, ignore_index=True)

        parts.append(long)

    out = pd.concat(parts, ignore_index=True)
    out = out[out["Fecha"] >= START_DATE].copy()
    out.rename(columns={"Fecha": "date"}, inplace=True)
    out["date"] = pd.to_datetime(out["date"])
    out["year"] = out["date"].dt.year
    out["month"] = out["date"].dt.month
    out["day"] = out["date"].dt.day
    out = out.sort_values(["date", "central"]).reset_index(drop=True)
    return out

def build_hidrologia_diaria_larga() -> pd.DataFrame:
    files = sorted(HIDRO_DIR.glob("*.csv"))
    if not files:
        raise FileNotFoundError(f"No CSV files found in {HIDRO_DIR}")

    parts = []
    numeric_cols = list(HIDRO_COLS.keys())

    for f in files:
        df = _read_csv(f)
        df = _parse_fecha(df)

        required = ["Fecha", *numeric_cols]
        missing = [c for c in required if c not in df.columns]
        if missing:
            raise ValueError(f"{f.name} missing columns: {missing}")

        # Placeholder rule: all hydro numeric cols are exactly 0 (not NaN)
        df["is_placeholder"] = df.apply(lambda r: 1 if _is_all_zero_row(r, numeric_cols) else 0, axis=1)

        rows = []
        for col, (central, kind) in HIDRO_COLS.items():
            tmp = df[["Fecha", "is_placeholder", col]].copy()
            tmp.rename(columns={col: "value"}, inplace=True)
            tmp["central"] = central
            tmp["kind"] = kind
            tmp.loc[tmp["is_placeholder"] == 1, "value"] = pd.NA
            rows.append(tmp)

        long = pd.concat(rows, ignore_index=True)
        parts.append(long)

    out = pd.concat(parts, ignore_index=True)
    out = out[out["Fecha"] >= START_DATE].copy()
    out.rename(columns={"Fecha": "date"}, inplace=True)
    out["date"] = pd.to_datetime(out["date"])
    out["year"] = out["date"].dt.year
    out["month"] = out["date"].dt.month
    out["day"] = out["date"].dt.day
    out = out.sort_values(["date", "central", "kind"]).reset_index(drop=True)
    return out

def main() -> int:
    prod = build_produccion_diaria_larga()
    hidro = build_hidrologia_diaria_larga()

    prod_out = OUT_DIR / "produccion_diaria_larga.csv"
    hidro_out = OUT_DIR / "hidrologia_diaria_larga.csv"

    prod.to_csv(prod_out, index=False)
    hidro.to_csv(hidro_out, index=False)

    meta = {
        "project": "celec_hidro_produccion_dashboard",
        "start_date": str(START_DATE.date()),
        "last_date_produccion": str(pd.to_datetime(prod["date"]).max().date()),
        "last_date_hidrologia": str(pd.to_datetime(hidro["date"]).max().date()),
        "generated_utc": dt.datetime.utcnow().isoformat(timespec="seconds") + "Z",
        "notes": [
            "produccion_diaria_larga: energia_mwh daily per central, long format. CSR is computed as Molino+Mazar+Sopladora+MSF.",
            "hidrologia_diaria_larga: value daily per central/kind (caudal_m3s or cota_msnm), long format.",
            "Placeholder rows: when ALL numeric columns for that day are exactly 0, values are set to NA and is_placeholder=1."
        ],
    }
    (OUT_DIR / "meta.json").write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")

    print("[OK] Wrote datasets to ./data")
    return 0

if __name__ == "__main__":
    raise SystemExit(main())
